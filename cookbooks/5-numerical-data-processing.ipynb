{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с числовыми данными\n",
    "Количественные данные служат для измерения чего-либо — будь то размер класса, ежемесячные продажи или оценки учащихся. Естественным способом представления этих величин является числовое представление (например, 29 студентов, 234 876 ₽ продаж.   \n",
    "Рассмотрим некоторые стратегии преобразования сырых числовых данных в признаки, целеноправленно формируемые для алгоритмов машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Шкалирование признака\n",
    "##### Задача\n",
    "Требуется шкалировать числовой признак в диапазон между двумя значениями\n",
    "##### Решение\n",
    "Для шкалирования признаков используем класс `MinMaxScaler` библиотеки `Scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28389109],\n",
       "       [0.35486387],\n",
       "       [0.42583664],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Создать признак\n",
    "feature = np.array([\n",
    "    [-500.5],\n",
    "    [-100.1],\n",
    "    [0],\n",
    "    [100.1],\n",
    "    [909.9]\n",
    "])\n",
    "\n",
    "# Создать шкалировщик\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Прошкалировать признак\n",
    "scaled_feature = minmax_scale.fit_transform(feature)\n",
    "\n",
    "# Показать прошкалированный признак\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шкалирование** — это общепринятая задача предобработки данных в машинном обучении. Многие алгоритмы МО исходят из того, что все признаки находятся на одинаковой шкале, как правило от `0` до `1` или от `-1` до `1`. Существует целый ряд методов шкалирования, но один из самых простых называется *минимаксным шкалированием*. В минимаксном шкалировании минимальное и максимальное значения признака используют для шкалирования значений внутри диапазона.    В частности минимаксвычисляется следующим образом:  \n",
    "$$x^`_i = \\frac{x_i - min(x)}{max(x) - min(x)}$$\n",
    "где $x$ – это вектор признака, $x_i$ – отдельный элемент признака $x$, $x^`_i$ – прошкалированный элемент.    \n",
    "\n",
    "В данном примере из выведенного массива видно, что признак ьыл успешно прошкалирован в диапазон от 0 до 1. Установить дианазон позволяет аргумент конструктора класса `MinMaxScaler` `feature_range=`.    \n",
    "Класс библиотеки scikit-learn `MinMaxScaler` предлагает два варианта шкалирования признака:\n",
    "* первый вариант – использовать метод `fit()` для вычисления минимального и максимального значения признака, а затем применить метод `transform()` для шкалирования;\n",
    "* второй вариант – вызвать метод `fit_transform()` для выполнения обеих операций одновременно.   \n",
    "\n",
    "Между этими двумя вариантаминет никакой математической разницы, но иногда есть практическая выгода в том, чтобы разделить эти операции, потому как это позволяет применять одно и тоже преобразование к разным наборам данных.\n",
    "#####  Дополнительные материалы\n",
    "* \"Шкалирование признаков\", Википедия: https://ru.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D0%B5_%D1%88%D0%BA%D0%B0%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5     \n",
    "* Себастьян Рашка \"О шкалировании и нормализации признаков\": http://sebastianraschka.com/Articles/2014_about_feature_scaling.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Стандартизация признака\n",
    "##### Задача\n",
    "Требуется преобразовать признак, чтобы он имел среднее значение равное `0` и стандартное отклонение равное `1`\n",
    "##### Решение\n",
    "Используем класс `StandardScaler` библиотеки `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76058269],\n",
       "       [-0.54177196],\n",
       "       [-0.35009716],\n",
       "       [-0.32271504],\n",
       "       [ 1.97516685]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать признак\n",
    "x = np.array([\n",
    "    [-1000.1],\n",
    "    [-200.2],\n",
    "    [500.5],\n",
    "    [600.6],\n",
    "    [9000.9]\n",
    "])\n",
    "\n",
    "# Создать шкалировщик\n",
    "standard_scale =preprocessing.StandardScaler()\n",
    "\n",
    "# Преобразовать признак\n",
    "standardized = standard_scale.fit_transform(x)\n",
    "\n",
    "# Показать признак\n",
    "standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распространенной альтернативой минимаксному шкалированию является шкалирование признаков, при котором они должны быть приближены к стандартному распределению. Для этого используется стандартизация, в ходе которой данные преобразуются таким образом, что они имееют среднее значение $\\bar{x} = 0$ и стандартное отклонение $\\sigma = 1$.   \n",
    "В частности каждый элемент в признаке преобразуется таким образом, чтобы:   \n",
    "$$x^`_i = \\frac{x_i - x}{\\sigma}$$\n",
    "где $x^`_i$ – наша тандартизированная форма $x_i$. Преобразованный признак представляет собой количество стандартных отклонений, на которое исходное значение отстоит от среднего значения признака – так называемая *$z$-оценка* в статистике.    \n",
    "В ммашинном обучении стандартизация является распространенным методом шкалирования с челью предобработки и на практике используется чаще, чем минимаксное шкалирование. Однако выбор варианта шкалирования признаков зависит от обучающегося алгоритма. Например, метод главных компонент часто работает лучше с использованием стандартизации, в то время как для нейронных сетей часто рекомендуется минимаксное шкалирование. В качестве общего правила, если нет причин использовать конкретный тип шкалирования, лучше примениять страндартизацию.     \n",
    "Можно видеть эффект стандартизации, обратившись к среднему значению и стандартному отклонению результата решения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее:  0.0\n",
      "Стандартное отклонение:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Напечатать среднее значение и стандартное отклонение\n",
    "print('Среднее: ', round(standardized.mean()))\n",
    "print('Стандартное отклонение: ', standardized.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если данные имеют значительные выбросы, то это может негативно повлиять на стандартизацию, сказываясь на среднем значении и дисперсии признаков. В таком случае часто бывает полезно проанализировать признаки, используя медиану и межквартильный размах. В библиотеки `scikit-learn` для этого используется  класс `RobustScaler`, реализующий метод робастного шкалирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87387612],\n",
       "       [-0.875     ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [10.61488511]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать шкалировщик\n",
    "robust_scale = preprocessing.RobustScaler()\n",
    "\n",
    "# Преобразовать признак\n",
    "robust_scale.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Нормализация наблюдений\n",
    "##### Задача\n",
    "Требуется прошкалировать значения признаков в наблюдениях для получения единичной нормы (общей длиной 1)\n",
    "##### Решение\n",
    "Изспользуем класс `Normolizer` с аргументом `norm=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.40364021, 0.9149178 ],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Создать матрицу признаков\n",
    "features = np.array([\n",
    "    [0.5, 0.5],\n",
    "    [1.1, 3.4],\n",
    "    [1.5, 3.4],\n",
    "    [1.63, 34.4],\n",
    "    [10.9, 3.3]\n",
    "])\n",
    "\n",
    "# Создать нормализатор\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# Преобразовать матрицу признаков\n",
    "normalizer.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие методы шкалирования, напрмер минимаксное шкалирование и стандартизация, работают с признаками; однако можно шкалировать и отдельные наблюдения. Класс `Normalizer` шкалирует значения в отдельных наблюдениях, приводя их к единичной норме (сумма их длин равна 1). Этот тип шкалирования часто используют, когда имеется много эквивалентных признаков, например в классификации текста, где каждое слово или группа $n$-слоев является признаком.    \n",
    "Класс `Normolizer` предоставляет три варианта нормы, приэтом евклидова норма (нередко именуемая $L^2$-нормой) является аргументом по умолчанию.    \n",
    "$$||x||_2 = \\sqrt{x^2_1 + x^2_2 + \\ldots + x^2_n}$$\n",
    "где $x$ – отдельное наблюдение; $x_n$ – значение этого наблюдения для $n$-го признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.40364021, 0.9149178 ],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пребразовать матрицу признаков\n",
    "features_l2_norm = Normalizer(norm='l2').transform(features)\n",
    "\n",
    "# Показать матрицу признкаов\n",
    "features_l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве альтернативы можно указать манхэттенскую норму ($L^1$):\n",
    "$$||x||_1 = \\sum_{i=1}^n{|x_i|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.24444444, 0.75555556],\n",
       "       [0.30612245, 0.69387755],\n",
       "       [0.04524008, 0.95475992],\n",
       "       [0.76760563, 0.23239437]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Преобразовать матрицу признаков\n",
    "features_l1_norm = Normalizer(norm='l1').transform(features)\n",
    "\n",
    "# Показать матрицу признаков\n",
    "features_l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуитивно норму $L^2$ можно воспринимать как расстояние между двумя точка в Нью-Йорке, пролетаемое птицей (т.е. по прямой), в то время как $L^1$ можно воспринимать как расстояние между теме же точками, но пройденное человеком по улицам, поэтому такое название \"манхэттенская норма\" или \"таксомоторная норма\".     \n",
    "На практике заметим, что `norm=l1` шкалирует значения наблюдения таким образом, что в сумме они дают `1`. Иногда такая сумма может быть желательным качеством."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сумма значений первого наблюдения:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Напечатать сумму\n",
    "print(\"Сумма значений первого наблюдения: \", features_l1_norm[0][0] + features_l1_norm[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Генерирование полиномиальных и взаимодействующих признаков\n",
    "##### Задача\n",
    "Требуется создать полиномиальные и взаимодействующие признаки\n",
    "##### Решение\n",
    "Используем класс `PolynomialFeatures` библиотеки `scikit=learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Создать матрицу признаков\n",
    "features = np.array([\n",
    "    [2, 3],\n",
    "    [2, 3],\n",
    "    [2, 3]\n",
    "])\n",
    "\n",
    "# Создать объект PolynomialFeatures\n",
    "polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Создать полиномиальные признаки\n",
    "polynomial_interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `degree=` определяет максимальный порядок полинома. Например, `degree=2` создаст новые признаки, возведенные во вторую степень:\n",
    "$$x_1, x_2, x^2_1, x^2_2,$$\n",
    "в то время как `degree=3` создаст новые признаки возведенные во вторую и в третью степени:\n",
    "$$x_1, x_2, x^2_1, x^2_2, x^3_1, x^3_2.$$\n",
    "Более того, как по умолчанию метод, реализованный в класса `PolynomialFeatures` включает в себя признаки взаимодействия:\n",
    "$$x_1, x_2.$$\n",
    "Можно ограничить создаваемые признаки только признаками взаимодействия, установив для `interaction_only=` значение `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 6.],\n",
       "       [2., 3., 6.],\n",
       "       [2., 3., 6.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полиномиальные признаки часто создаются, когда предполагается, что существует нелинейная связь между признаками и целью. Например, можно подозревать, что влияние возраста на вероятность наличия серьезных заболеваний не является постоянным с течением времени, а возрастает по мере увеличения возраста. Можно закодировать этот неконстантный эффект в признаке $x$, генерируя формы признака более высокого порядка – $x^2$, $x^3$ и т.д.   \n",
    "\n",
    "Кроме того, нередко приходится сталкиваться с ситуациями, когда эффект одного признака зависит от другого признака. Просты примером была попытка предсказать является ли кофе сладким при следующих двух признаках:\n",
    "1) был ли кофе перемешан;\n",
    "2) добавляли ли в него сахар.    \n",
    "\n",
    "Отдельно каждый признак сладость кофе не предсказывает, но соченание признаков – это делает. Т.е. кофе будет сладким, если в нем есть сахар и он перемешан. Влияние кадого признака на цель (сладость) зависит друг от друга. Такую связь можно кодировать, включив взаимодействующий признак, который является произведением отдельных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Преобразование признаков\n",
    "##### Задача\n",
    "Требуется выполнить собственное преобразование одного или нескольких признаков\n",
    "##### Решение\n",
    "Используем класс `FunctionTransformer` библиотеки `scikit-learn` для применения функции к набору признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12, 13],\n",
       "       [12, 13],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Создать матрицу признаков\n",
    "features = np.array([\n",
    "    [2, 3],\n",
    "    [2, 3],\n",
    "    [2, 3]\n",
    "])\n",
    "\n",
    "# Определить простую функцию\n",
    "def add_ten(n):\n",
    "    return n + 10\n",
    "\n",
    "# Создать преобразователь\n",
    "ten_transformer = FunctionTransformer(add_ten)\n",
    "\n",
    "# Преобразовать матрицу признаков\n",
    "ten_transformer.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативный вариант – использовать функцию `apply()` библиотеки `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Признак 1</th>\n",
       "      <th>Признак 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Признак 1  Признак 2\n",
       "0         12         13\n",
       "1         12         13\n",
       "2         12         13"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "import pandas as pd \n",
    "\n",
    "# Создать фрейм данных\n",
    "df = pd.DataFrame(data=features, columns=['Признак 1', 'Признак 2'])\n",
    "\n",
    "# Применить функцию\n",
    "df.apply(add_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто возникает необходимость выполнить определенные собственные преобразования для одного или более признаков. Например, мы можем создать признак, который является натуральным логарифмом значений другого признака. Сделать это можно создав функцию, а затем применив ее на признаки с помощью либо класса `FunctionTransform` библиотеки `scikit-learn`, либо в помощью метода `apply()` библиотеки`pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Обнаружение выбросов\n",
    "##### Задача\n",
    "Требуется индетифицировать предельные значения\n",
    "##### Решение\n",
    "Используем класс `EllipticEnvelope` библиотеки `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обнаружение выбросов часто больше искусство, чем наука. Вместе с тем распространенным методом является принятие допущения о том, что данные норально распределены. Основываясь на этом допущении мы можем \"рисовать\" эллипс вокруг данных, классифицируя любое наблюдение внутри эллипса как не выброс (помечаем как `1`) и любое наблюдение за пределами эллипса как выброс (помечаем как `-1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_blobs \n",
    "\n",
    "# Создать симулированные данные\n",
    "features, _ = make_blobs(n_samples=10, n_features=2, centers=1, random_state=1)\n",
    "\n",
    "# Заменить значения первого наблюдения предельными значениями\n",
    "features[0, 0] = 100000\n",
    "features[0, 1] = 200000\n",
    "\n",
    "# Создать детектор\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "\n",
    "# Выполнить подгонку детектора\n",
    "outlier_detector.fit(features)\n",
    "\n",
    "# Предсказать выбросы\n",
    "outlier_detector.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явным ограничением этого подхода является необходимость указания парметра загрязнения `contamination=`, который представляет собой долю наблюдений являющихся выбросами — значение, которое мы не знаем. Можно думать о загрязнении как о собственной оценке чистоты данных. Если ожидается, что исследуемые данные будут иметь несколько выбросов, можно задать параметр `contamination=` с каким-нибудь небольшим значением. Однако если данные имеют большое количество выбросов, то для данного параметра можно установить более высокое значение.\n",
    "\n",
    "Вместо того, чтобы смотреть на наблюдения в целом, можно взглнять на отдельные признаки и индетифицировать в этих признаках предельные значения, используя межквартильный размах (МКР, IQR):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать один признак\n",
    "feature = features[:,0]\n",
    "\n",
    "# Создать функцию, которая возвращает индекс выбросов\n",
    "def indicates_of_outliers(x):\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    return np.where((x > upper_bound) | (x < lower_bound))\n",
    "\n",
    "# Выполнить функцию\n",
    "indicates_of_outliers(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Межквартильный размах – это разница между первым и третьим квартилями набора данных. МКР можно представить как разброс основной части данных, где выбросы отдаленные от соновной части данных. Выбросы обычно определяются как любое значение, которое на 1.5 МКР меньше первого квартиля или на 1.5 МКР больше третьего квартиля.\n",
    "\n",
    "Единого наилучшего метода оьнаружения выбросов не существует. Вместо этого у нас есть коллекция методов, все со своими преимуществами и недостатками. Часто лучшая стратегия состоит в том, чтобы пытаться использовать несколько методов, например: и `EllipticEnvelop`, и обнаружение на основе МКР.\n",
    "\n",
    "Если это возможно необходимо посмотреть на те наблюдения, которые были характеризованы как выбросы, и попытаться их понять. Например, возьмем набор данных о домах, в котором один из признаков является количество комнат. Является ли выброс со 100 комнатами действительно домом или это на самом деле отель, который был неправильно классифицирован?\n",
    "\n",
    "##### Дополнительные материалы\n",
    "* Статья \"Три способа обнаружения выбросов\": http://colingorrie.github.io/outlier-detection.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Обработка выбросов\n",
    "##### Задача\n",
    "Имеются выбросы\n",
    "##### Решение\n",
    "Для обработки выбросов можно использовать три стратегии:   \n",
    "1) отбросить выбросы;   \n",
    "2) пометить как выбросы и включить их в качестве признака;   \n",
    "3) преобразовать признак, чтобы ослабить эффект выброса    \n",
    "\n",
    "***1) Отбросить выбросы***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Цена</th>\n",
       "      <th>Ванные</th>\n",
       "      <th>Кв_футы</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>392298</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1238823</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Цена  Ванные  Кв_футы\n",
       "0   534433     2.0     1500\n",
       "1   392298     3.5     2500\n",
       "2  1238823     2.0     1500"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать фрейм данных\n",
    "houses = pd.DataFrame()\n",
    "houses['Цена'] = [534433, 392298, 1238823, 4322032]\n",
    "houses['Ванные'] = [2, 3.5, 2, 116]\n",
    "houses['Кв_футы'] = [1500, 2500, 1500, 48000]\n",
    "\n",
    "# Отфильтровать наблюдения\n",
    "houses[houses['Ванные'] < 20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2) Включить выбросы как признак***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Цена</th>\n",
       "      <th>Ванные</th>\n",
       "      <th>Кв_футы</th>\n",
       "      <th>Выброс</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>392298</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1238823</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Цена  Ванные  Кв_футы  Выброс\n",
       "0   534433     2.0     1500       0\n",
       "1   392298     3.5     2500       0\n",
       "2  1238823     2.0     1500       0\n",
       "3  4322032   116.0    48000       1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать признак на основе услвного выражения\n",
    "houses['Выброс'] = np.where(houses['Ванные'] < 20, 0, 1)\n",
    "\n",
    "# Показать данные\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3) Преобразовать выброс, ослабить его эффект***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Цена</th>\n",
       "      <th>Ванные</th>\n",
       "      <th>Кв_футы</th>\n",
       "      <th>Выброс</th>\n",
       "      <th>Логарифм кв_футов</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>392298</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1238823</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.778956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Цена  Ванные  Кв_футы  Выброс  Логарифм кв_футов\n",
       "0   534433     2.0     1500       0           7.313220\n",
       "1   392298     3.5     2500       0           7.824046\n",
       "2  1238823     2.0     1500       0           7.313220\n",
       "3  4322032   116.0    48000       1          10.778956"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взять логарифм признака\n",
    "houses['Логарифм кв_футов'] = [np.log(x) for x in houses['Кв_футы']]\n",
    "\n",
    "# Показать данные\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобно обнаружению выбросов, заведенных правил обработки выбросов не существует. В общем случа стратегия должна основываться на двух аспектах:\n",
    "* во-первых, необходимо понять, что делает данные выбросами. Если выбросы идентифицированны как ошибки в данных, например, из-за сломанного датчика или неверного значения, то необходимо исключить это наблюдение или заменить значения выбросов на `NaN`, т.к. этим значениям нельзя верить. Однако, допустить, что выбросы являются подлинными предельными значениями (нпример, дом с 200 ванными), то более уместной будет маркировка их как выбросы или преобразование их значений;\n",
    "* во-вторых, стратегия обраотки выбросов должна основываться целях машинного обучения. Например, если необходимо предсказать цены на жилье на основе признаков дома, то было бы разумно предположить, что цена на особняки с более чем 100 ванными комнатами обусловлена другой динамикой, чем обучные семейные дома. Кроме того, если производится подготовка модели для использования в качестве части веб-приложения онлайн-кредитования на жилье, то можно предположить, что среди клиентов не будет миллиардеров, желающих купить особняк.\n",
    "\n",
    "В общем виде алгоритм по работе с выбросами такой: \n",
    "* необходимо определить почему выбросы являются выбросами;\n",
    "* необходимо держать в уме цели обработки набора данных для конкретной задачи машинного обучения.\n",
    "\n",
    "Само по себе непринятие решения об удалении или работе в выбросами само по себе является решением с последствиями.\n",
    "Если в данных имеются выбросы, то, напрмер, стандартизация данных может оказаться неуместной, посколькусреднее значение и дисперсия сильно зависят от выбросов. В таком случае необходимо использовать более устойчивый в выбросам метод, например `RobustScaler`.\n",
    "\n",
    "##### Дополнительные материалы\n",
    "* Документация по робастному шкалировщику `RobustScaler`: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
