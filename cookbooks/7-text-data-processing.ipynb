{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с текстом\n",
    "Неструктурированные текстовые данные, такие как содержимое книги или твита, являются одними из самых интересных источников признаков и одними из самых сложных для обработки. Рассмотрим стратегии преобразования текста в информационно богатые признаки. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Очистка текста\n",
    "##### Задача\n",
    "Даны некоторые неструктурированные текстовые данные, требуется выполнить их элементарную очистку\n",
    "##### Решение\n",
    "Используем элементарные операции языка Python над строковыми значениями: `strip()`, `replace()`, `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Война и мир. Лев Толстой',\n",
       " 'Идиот. Федор Достоевский',\n",
       " 'Узник замка Иф. Александр Дюма']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать текст\n",
    "text_data = [\n",
    "    '  Война и мир. Лев Толстой',\n",
    "    'Идиот. Федор Достоевский   ',\n",
    "    ' Узник замка Иф. Александр Дюма ',\n",
    "]\n",
    "\n",
    "# Удалить пробелы\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "# Показать текст\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Война и мир Лев Толстой',\n",
       " 'Идиот Федор Достоевский',\n",
       " 'Узник замка Иф Александр Дюма']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удалить точки\n",
    "remove_periods = [string.replace('.', '') for string in strip_whitespace]\n",
    "\n",
    "# Показать текст\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно создать собственную функцию преобразования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ВОЙНА И МИР ЛЕВ ТОЛСТОЙ',\n",
       " 'ИДИОТ ФЕДОР ДОСТОЕВСКИЙ',\n",
       " 'УЗНИК ЗАМКА ИФ АЛЕКСАНДР ДЮМА']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать функцию\n",
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "# Применить функцию\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения строковых операций можно воспользоваться регулярными выражениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXX X XXX XXX XXXXXXX',\n",
       " 'XXXXX XXXXX XXXXXXXXXXX',\n",
       " 'XXXXX XXXXX XX XXXXXXXXX XXXX']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импортировать библиотеку\n",
    "import re \n",
    "\n",
    "# Создать функцию\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[а-яА-я]\", \"X\", string)\n",
    "\n",
    "# Применить функцию\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство текстовых данных требуется очистить перед тем, как их использовать для построения признаков. Подавляющую часть элементарной очистки текста можно выполнять м помощью стандартных строковых операций Python. На практике имеет смысл разработать собственные функции очистки и включить их в общий pipeline анализа данных.\n",
    "##### Дополнительные материалы\n",
    "* [\"Практическое руководство по регулярным выражениям в Python\"](https://www.analyticsvidhya.com/blog/2015/06/regular-expression-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Разбор и очистка разметки HTML\n",
    "##### Задача \n",
    "Даны текстовые данные с элементами HTML, требуется извлечь только текст\n",
    "##### Решение\n",
    "Используем набор функциональных средств библиотеки `Beautiful Soup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global earthquake activity since 1973 and nuclear power plant locations'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Создать код HTML\n",
    "html = \"\"\"\n",
    "        <div id=\"shell\">\n",
    "               <h3>\n",
    "                  Global earthquake activity since 1973 and nuclear power plant locations\n",
    "               </h3>\n",
    "        </div>\n",
    "       \"\"\"\n",
    "# Выполнить разбор HTML\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Найти элемент div с id=shell, показать текст\n",
    "soup.find('div', {'id': 'shell'}).text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` мощная библиотека Python, предназначенная для \"выскабливания\" HTML. Как правило ее используют для вычищения \"живых\" веб-сайтов. Но ее также можно применять для извлечения текстовых данных, встроенных в HTML.\n",
    "##### Дополнительные материалы\n",
    "* [Библиотека BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Удаление знаков препинания\n",
    "##### Задача\n",
    "Дан признак в текстовых данных, и требуется кдалить знаки препинания\n",
    "##### Решение\n",
    "Используем функцию `translate()` со словарем знаков препинания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Разнообразный и богатый опыт говорит нам ',\n",
       " 'что высокотехнологичная концепция общественного уклада ',\n",
       " 'является качественно новой ступенью анализа существующих паттернов поведения ',\n",
       " 'Таким образом убеждённость некоторых оппонентов напрямую',\n",
       " 'зависит от распределения внутренних резервов и ресурсов']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# Создать текст\n",
    "text_data = [\n",
    "    'Разн!!ообразный и богатый оп,.;ыт говорит нам, ',\n",
    "    'что в.,.ысокотехнологичная концепция общественного уклада ',\n",
    "    'является ка;чествен%но новой ступенью анализа существующих паттернов??? поведения. ',\n",
    "    'Таким образом, убеждённо*сть некоторых оппо?!нентов напрямую',\n",
    "    'зависит от распред:,.еления внутренних резервов ::и ресурсов.',\n",
    "]\n",
    "\n",
    "# Создать словарь знаков препинания\n",
    "punctuation = dict.fromkeys(\n",
    "    i for i in range(sys.maxunicode)\n",
    "    if unicodedata.category(chr(i)).startswith('P')\n",
    ")\n",
    "\n",
    "# Удалить любые знаки препинания во всех строковых значениях\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `translate()` языка Python популярен благодаря невероятной скорости. В данном решение сначала был создан словарь `punctuation`, в котором в качестве ключей размещены все знаки препинания, присутствующие в Юникоде, и в качестве значений – `None`. Затем все символы строкового значения, которые находились среди значков препинания были преобразованы в `None`, фактически удалив их.\n",
    "\n",
    "Существуют и более читаемые способы удаления знаков препинания, но данное решение значительно быстрее работает, чем остальные.\n",
    "\n",
    "Важно помнить, что знаки препинания содержат информацию. Например: \"правильно?\" или \"правильно!\". Удаление знаков препинания часто является необходимым злом для создания признаков; однако если знаки препинания важны, мы должны принимать их во внимание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Лексемизация текста\n",
    "##### Задача\n",
    "Дан текст, и тербуется развить его на отдельные слова\n",
    "##### Решение\n",
    "Используем комплект естественно-языковых инструментов NLTK (Natural Language Toolkit for Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Сегодняшняя', 'наука', '—', 'это', 'технология', 'завтрашнего', 'дня']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Создать текст\n",
    "string = 'Сегодняшняя наука — это технология завтрашнего дня'\n",
    "\n",
    "# Лексемизировать слова\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лексимизация, в особенности лексимизация на слова, является распространенной задачей после очистки текстовых данных, потому что это первый шаг в процессе превращения текста в данные, которые можно использовать для создания полезных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Удаление стоп-слов\n",
    "##### Задача\n",
    "Имеются лексимизированные текстовые данные, из которых требуется удалить частые обще употребимые слова\n",
    "##### Решение\n",
    "Используем функцию `stopwords()` библиотеки `NLTK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/antonneverovich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Внезапно',\n",
       " ',',\n",
       " 'непосредственные',\n",
       " 'участники',\n",
       " 'технического',\n",
       " 'прогресса',\n",
       " 'являются',\n",
       " 'методом',\n",
       " 'политического',\n",
       " 'участия']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Сформировать набор стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "# Создать лексемы слов\n",
    "text_data = \"\"\"\n",
    "    Внезапно, непосредственные участники технического прогресса являются только методом политического участия и \n",
    "    смешаны с не уникальными данными до степени совершенной неузнаваемости, из-за чего возрастает их статус \n",
    "    бесполезности. Вот вам яркий пример современных тенденций - высокотехнологичная концепция общественного уклада\n",
    "    влечет за собой процесс внедрения и модернизации приоритизации разума над эмоциями. Но выбранный нами \n",
    "    инновационный путь не оставляет шанса для дальнейших направлений развития.\n",
    "    \"\"\"\n",
    "\n",
    "tokenized_words = word_tokenize(text_data)\n",
    "\n",
    "# Удалить стоп-слова\n",
    "[word for word in tokenized_words if word not in stop_words][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя стоп-слова могут относиться к любому набору слов, которые мы хотим удалить перед обработкой, часто этот термин относится к чрезвычайно распространенным словам, которые сами по себе содержат мало информации. NLTK имеет список общеупотребимых стоп-слов, который можно использовать для их отыскания и удаления в наших лексимизированные словах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удалить стоп-слова\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следует обратить внимание, что функция `stopwords` библиотеки NLTK исходит из того, ято все лексимизируемые слова находятся в нижнем регистре."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
