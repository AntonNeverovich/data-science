{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с числовыми данными\n",
    "Количественные данные служат для измерения чего-либо — будь то размер класса, ежемесячные продажи или оценки учащихся. Естественным способом представления этих величин является числовое представление (например, 29 студентов, 234 876 ₽ продаж.   \n",
    "Рассмотрим некоторые стратегии преобразования сырых числовых данных в признаки, целеноправленно формируемые для алгоритмов машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Шкалирование признака\n",
    "##### Задача\n",
    "Требуется шкалировать числовой признак в диапазон между двумя значениями\n",
    "##### Решение\n",
    "Для шкалирования признаков используем класс `MinMaxScaler` библиотеки `Scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28389109],\n",
       "       [0.35486387],\n",
       "       [0.42583664],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Создать признак\n",
    "feature = np.array([\n",
    "    [-500.5],\n",
    "    [-100.1],\n",
    "    [0],\n",
    "    [100.1],\n",
    "    [909.9]\n",
    "])\n",
    "\n",
    "# Создать шкалировщик\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Прошкалировать признак\n",
    "scaled_feature = minmax_scale.fit_transform(feature)\n",
    "\n",
    "# Показать прошкалированный признак\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шкалирование** — это общепринятая задача предобработки данных в машинном обучении. Многие алгоритмы МО исходят из того, что все признаки находятся на одинаковой шкале, как правило от `0` до `1` или от `-1` до `1`. Существует целый ряд методов шкалирования, но один из самых простых называется *минимаксным шкалированием*. В минимаксном шкалировании минимальное и максимальное значения признака используют для шкалирования значений внутри диапазона.    В частности минимаксвычисляется следующим образом:  \n",
    "$$x^`_i = \\frac{x_i - min(x)}{max(x) - min(x)}$$\n",
    "где $x$ – это вектор признака, $x_i$ – отдельный элемент признака $x$, $x^`_i$ – прошкалированный элемент.    \n",
    "\n",
    "В данном примере из выведенного массива видно, что признак ьыл успешно прошкалирован в диапазон от 0 до 1. Установить дианазон позволяет аргумент конструктора класса `MinMaxScaler` `feature_range=`.    \n",
    "Класс библиотеки scikit-learn `MinMaxScaler` предлагает два варианта шкалирования признака:\n",
    "* первый вариант – использовать метод `fit()` для вычисления минимального и максимального значения признака, а затем применить метод `transform()` для шкалирования;\n",
    "* второй вариант – вызвать метод `fit_transform()` для выполнения обеих операций одновременно.   \n",
    "\n",
    "Между этими двумя вариантаминет никакой математической разницы, но иногда есть практическая выгода в том, чтобы разделить эти операции, потому как это позволяет применять одно и тоже преобразование к разным наборам данных.\n",
    "#####  Дополнительные материалы\n",
    "* \"Шкалирование признаков\", Википедия: https://ru.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D0%B5_%D1%88%D0%BA%D0%B0%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5     \n",
    "* Себастьян Рашка \"О шкалировании и нормализации признаков\": http://sebastianraschka.com/Articles/2014_about_feature_scaling.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Стандартизация признака\n",
    "##### Задача\n",
    "Требуется преобразовать признак, чтобы он имел среднее значение равное `0` и стандартное отклонение равное `1`\n",
    "##### Решение\n",
    "Используем класс `StandardScaler` библиотеки `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76058269],\n",
       "       [-0.54177196],\n",
       "       [-0.35009716],\n",
       "       [-0.32271504],\n",
       "       [ 1.97516685]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать признак\n",
    "x = np.array([\n",
    "    [-1000.1],\n",
    "    [-200.2],\n",
    "    [500.5],\n",
    "    [600.6],\n",
    "    [9000.9]\n",
    "])\n",
    "\n",
    "# Создать шкалировщик\n",
    "standard_scale =preprocessing.StandardScaler()\n",
    "\n",
    "# Преобразовать признак\n",
    "standardized = standard_scale.fit_transform(x)\n",
    "\n",
    "# Показать признак\n",
    "standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распространенной альтернативой минимаксному шкалированию является шкалирование признаков, при котором они должны быть приближены к стандартному распределению. Для этого используется стандартизация, в ходе которой данные преобразуются таким образом, что они имееют среднее значение $\\bar{x} = 0$ и стандартное отклонение $\\sigma = 1$.   \n",
    "В частности каждый элемент в признаке преобразуется таким образом, чтобы:   \n",
    "$$x^`_i = \\frac{x_i - x}{\\sigma}$$\n",
    "где $x^`_i$ – наша тандартизированная форма $x_i$. Преобразованный признак представляет собой количество стандартных отклонений, на которое исходное значение отстоит от среднего значения признака – так называемая *$z$-оценка* в статистике.    \n",
    "В ммашинном обучении стандартизация является распространенным методом шкалирования с челью предобработки и на практике используется чаще, чем минимаксное шкалирование. Однако выбор варианта шкалирования признаков зависит от обучающегося алгоритма. Например, метод главных компонент часто работает лучше с использованием стандартизации, в то время как для нейронных сетей часто рекомендуется минимаксное шкалирование. В качестве общего правила, если нет причин использовать конкретный тип шкалирования, лучше примениять страндартизацию.     \n",
    "Можно видеть эффект стандартизации, обратившись к среднему значению и стандартному отклонению результата решения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее:  0.0\n",
      "Стандартное отклонение:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Напечатать среднее значение и стандартное отклонение\n",
    "print('Среднее: ', round(standardized.mean()))\n",
    "print('Стандартное отклонение: ', standardized.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если данные имеют значительные выбросы, то это может негативно повлиять на стандартизацию, сказываясь на среднем значении и дисперсии признаков. В таком случае часто бывает полезно проанализировать признаки, используя медиану и межквартильный размах. В библиотеки `scikit-learn` для этого используется  класс `RobustScaler`, реализующий метод робастного шкалирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87387612],\n",
       "       [-0.875     ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [10.61488511]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать шкалировщик\n",
    "robust_scale = preprocessing.RobustScaler()\n",
    "\n",
    "# Преобразовать признак\n",
    "robust_scale.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Нормализация наблюдений\n",
    "##### Задача\n",
    "Требуется прошкалировать значения признаков в наблюдениях для получения единичной нормы (общей длиной 1)\n",
    "##### Решение\n",
    "Изспользуем класс `Normolizer` с аргументом `norm=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.40364021, 0.9149178 ],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Создать матрицу признаков\n",
    "features = np.array([\n",
    "    [0.5, 0.5],\n",
    "    [1.1, 3.4],\n",
    "    [1.5, 3.4],\n",
    "    [1.63, 34.4],\n",
    "    [10.9, 3.3]\n",
    "])\n",
    "\n",
    "# Создать нормализатор\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# Преобразовать матрицу признаков\n",
    "normalizer.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие методы шкалирования, напрмер минимаксное шкалирование и стандартизация, работают с признаками; однако можно шкалировать и отдельные наблюдения. Класс `Normalizer` шкалирует значения в отдельных наблюдениях, приводя их к единичной норме (сумма их длин равна 1). Этот тип шкалирования часто используют, когда имеется много эквивалентных признаков, например в классификации текста, где каждое слово или группа $n$-слоев является признаком.    \n",
    "Класс `Normolizer` предоставляет три варианта нормы, приэтом евклидова норма (нередко именуемая $L^2$-нормой) является аргументом по умолчанию.    \n",
    "$$||x||_2 = \\sqrt{x^2_1 + x^2_2 + \\ldots + x^2_n}$$\n",
    "где $x$ – отдельное наблюдение; $x_n$ – значение этого наблюдения для $n$-го признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.40364021, 0.9149178 ],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пребразовать матрицу признаков\n",
    "features_l2_norm = Normalizer(norm='l2').transform(features)\n",
    "\n",
    "# Показать матрицу признкаов\n",
    "features_l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве альтернативы можно указать манхэттенскую норму ($L^1$):\n",
    "$$||x||_1 = \\sum_{i=1}^n{|x_i|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.24444444, 0.75555556],\n",
       "       [0.30612245, 0.69387755],\n",
       "       [0.04524008, 0.95475992],\n",
       "       [0.76760563, 0.23239437]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Преобразовать матрицу признаков\n",
    "features_l1_norm = Normalizer(norm='l1').transform(features)\n",
    "\n",
    "# Показать матрицу признаков\n",
    "features_l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуитивно норму $L^2$ можно воспринимать как расстояние между двумя точка в Нью-Йорке, пролетаемое птицей (т.е. по прямой), в то время как $L^1$ можно воспринимать как расстояние между теме же точками, но пройденное человеком по улицам, поэтому такое название \"манхэттенская норма\" или \"таксомоторная норма\".     \n",
    "На практике заметим, что `norm=l1` шкалирует значения наблюдения таким образом, что в сумме они дают `1`. Иногда такая сумма может быть желательным качеством."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сумма значений первого наблюдения:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Напечатать сумму\n",
    "print(\"Сумма значений первого наблюдения: \", features_l1_norm[0][0] + features_l1_norm[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
