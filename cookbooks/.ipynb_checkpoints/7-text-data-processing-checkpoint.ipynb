{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с текстом\n",
    "Неструктурированные текстовые данные, такие как содержимое книги или твита, являются одними из самых интересных источников признаков и одними из самых сложных для обработки. Рассмотрим стратегии преобразования текста в информационно богатые признаки. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Очистка текста\n",
    "##### Задача\n",
    "Даны некоторые неструктурированные текстовые данные, требуется выполнить их элементарную очистку\n",
    "##### Решение\n",
    "Используем элементарные операции языка Python над строковыми значениями: `strip()`, `replace()`, `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Война и мир. Лев Толстой',\n",
       " 'Идиот. Федор Достоевский',\n",
       " 'Узник замка Иф. Александр Дюма']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать текст\n",
    "text_data = [\n",
    "    '  Война и мир. Лев Толстой',\n",
    "    'Идиот. Федор Достоевский   ',\n",
    "    ' Узник замка Иф. Александр Дюма ',\n",
    "]\n",
    "\n",
    "# Удалить пробелы\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "# Показать текст\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Война и мир Лев Толстой',\n",
       " 'Идиот Федор Достоевский',\n",
       " 'Узник замка Иф Александр Дюма']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удалить точки\n",
    "remove_periods = [string.replace('.', '') for string in strip_whitespace]\n",
    "\n",
    "# Показать текст\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно создать собственную функцию преобразования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ВОЙНА И МИР ЛЕВ ТОЛСТОЙ',\n",
       " 'ИДИОТ ФЕДОР ДОСТОЕВСКИЙ',\n",
       " 'УЗНИК ЗАМКА ИФ АЛЕКСАНДР ДЮМА']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать функцию\n",
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "# Применить функцию\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения строковых операций можно воспользоваться регулярными выражениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXX X XXX XXX XXXXXXX',\n",
       " 'XXXXX XXXXX XXXXXXXXXXX',\n",
       " 'XXXXX XXXXX XX XXXXXXXXX XXXX']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импортировать библиотеку\n",
    "import re \n",
    "\n",
    "# Создать функцию\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[а-яА-я]\", \"X\", string)\n",
    "\n",
    "# Применить функцию\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство текстовых данных требуется очистить перед тем, как их использовать для построения признаков. Подавляющую часть элементарной очистки текста можно выполнять м помощью стандартных строковых операций Python. На практике имеет смысл разработать собственные функции очистки и включить их в общий pipeline анализа данных.\n",
    "##### Дополнительные материалы\n",
    "* [\"Практическое руководство по регулярным выражениям в Python\"](https://www.analyticsvidhya.com/blog/2015/06/regular-expression-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Разбор и очистка разметки HTML\n",
    "##### Задача \n",
    "Даны текстовые данные с элементами HTML, требуется извлечь только текст\n",
    "##### Решение\n",
    "Используем набор функциональных средств библиотеки `Beautiful Soup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global earthquake activity since 1973 and nuclear power plant locations'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Создать код HTML\n",
    "html = \"\"\"\n",
    "        <div id=\"shell\">\n",
    "               <h3>\n",
    "                  Global earthquake activity since 1973 and nuclear power plant locations\n",
    "               </h3>\n",
    "        </div>\n",
    "       \"\"\"\n",
    "# Выполнить разбор HTML\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Найти элемент div с id=shell, показать текст\n",
    "soup.find('div', {'id': 'shell'}).text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` мощная библиотека Python, предназначенная для \"выскабливания\" HTML. Как правило ее используют для вычищения \"живых\" веб-сайтов. Но ее также можно применять для извлечения текстовых данных, встроенных в HTML.\n",
    "##### Дополнительные материалы\n",
    "* [Библиотека BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Удаление знаков препинания\n",
    "##### Задача\n",
    "Дан признак в текстовых данных, и требуется кдалить знаки препинания\n",
    "##### Решение\n",
    "Используем функцию `translate()` со словарем знаков препинания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Разнообразный и богатый опыт говорит нам ',\n",
       " 'что высокотехнологичная концепция общественного уклада ',\n",
       " 'является качественно новой ступенью анализа существующих паттернов поведения ',\n",
       " 'Таким образом убеждённость некоторых оппонентов напрямую',\n",
       " 'зависит от распределения внутренних резервов и ресурсов']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# Создать текст\n",
    "text_data = [\n",
    "    'Разн!!ообразный и богатый оп,.;ыт говорит нам, ',\n",
    "    'что в.,.ысокотехнологичная концепция общественного уклада ',\n",
    "    'является ка;чествен%но новой ступенью анализа существующих паттернов??? поведения. ',\n",
    "    'Таким образом, убеждённо*сть некоторых оппо?!нентов напрямую',\n",
    "    'зависит от распред:,.еления внутренних резервов ::и ресурсов.',\n",
    "]\n",
    "\n",
    "# Создать словарь знаков препинания\n",
    "punctuation = dict.fromkeys(\n",
    "    i for i in range(sys.maxunicode)\n",
    "    if unicodedata.category(chr(i)).startswith('P')\n",
    ")\n",
    "\n",
    "# Удалить любые знаки препинания во всех строковых значениях\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `translate()` языка Python популярен благодаря невероятной скорости. В данном решение сначала был создан словарь `punctuation`, в котором в качестве ключей размещены все знаки препинания, присутствующие в Юникоде, и в качестве значений – `None`. Затем все символы строкового значения, которые находились среди значков препинания были преобразованы в `None`, фактически удалив их.\n",
    "\n",
    "Существуют и более читаемые способы удаления знаков препинания, но данное решение значительно быстрее работает, чем остальные.\n",
    "\n",
    "Важно помнить, что знаки препинания содержат информацию. Например: \"правильно?\" или \"правильно!\". Удаление знаков препинания часто является необходимым злом для создания признаков; однако если знаки препинания важны, мы должны принимать их во внимание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Лексемизация текста\n",
    "##### Задача\n",
    "Дан текст, и тербуется развить его на отдельные слова\n",
    "##### Решение\n",
    "Используем комплект естественно-языковых инструментов NLTK (Natural Language Toolkit for Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Сегодняшняя', 'наука', '—', 'это', 'технология', 'завтрашнего', 'дня']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Создать текст\n",
    "string = 'Сегодняшняя наука — это технология завтрашнего дня'\n",
    "\n",
    "# Лексемизировать слова\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лексимизация, в особенности лексимизация на слова, является распространенной задачей после очистки текстовых данных, потому что это первый шаг в процессе превращения текста в данные, которые можно использовать для создания полезных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Удаление стоп-слов\n",
    "##### Задача\n",
    "Имеются лексимизированные текстовые данные, из которых требуется удалить частые обще употребимые слова\n",
    "##### Решение\n",
    "Используем функцию `stopwords()` библиотеки `NLTK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/antonneverovich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Внезапно',\n",
       " ',',\n",
       " 'непосредственные',\n",
       " 'участники',\n",
       " 'технического',\n",
       " 'прогресса',\n",
       " 'являются',\n",
       " 'методом',\n",
       " 'политического',\n",
       " 'участия']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Сформировать набор стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "# Создать лексемы слов\n",
    "text_data = \"\"\"\n",
    "    Внезапно, непосредственные участники технического прогресса являются только методом политического участия и \n",
    "    смешаны с не уникальными данными до степени совершенной неузнаваемости, из-за чего возрастает их статус \n",
    "    бесполезности. Вот вам яркий пример современных тенденций - высокотехнологичная концепция общественного уклада\n",
    "    влечет за собой процесс внедрения и модернизации приоритизации разума над эмоциями. Но выбранный нами \n",
    "    инновационный путь не оставляет шанса для дальнейших направлений развития.\n",
    "    \"\"\"\n",
    "\n",
    "tokenized_words = word_tokenize(text_data)\n",
    "\n",
    "# Удалить стоп-слова\n",
    "[word for word in tokenized_words if word not in stop_words][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя стоп-слова могут относиться к любому набору слов, которые мы хотим удалить перед обработкой, часто этот термин относится к чрезвычайно распространенным словам, которые сами по себе содержат мало информации. NLTK имеет список общеупотребимых стоп-слов, который можно использовать для их отыскания и удаления в наших лексимизированные словах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удалить стоп-слова\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следует обратить внимание, что функция `stopwords` библиотеки NLTK исходит из того, ято все лексимизируемые слова находятся в нижнем регистре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Выделение основ слов\n",
    "##### Задача\n",
    "Даны лексемизированные слова, требуется преобразовать их в корневые формы\n",
    "##### Решение\n",
    "Используем класс `PorterStemmer` библиотеки NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Создать лексемы слов\n",
    "tokenized_words = [\n",
    "    'i',\n",
    "    'am',\n",
    "    'humbled',\n",
    "    'by',\n",
    "    'this',\n",
    "    'traditional',\n",
    "    'meeting',\n",
    "]\n",
    "\n",
    "# Создать стеммер\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# применить стеммер\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для слов русского языка имспользуется стеммер `Snowball`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рыбак', 'рыбач', 'на', 'рек', 'и', 'пойма', 'больш', 'рыб']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Создать лексемы слов\n",
    "tokenized_words_rus = [\n",
    "    'Рыбаки',\n",
    "    'рыбачили',\n",
    "    'на',\n",
    "    'реке',\n",
    "    'и',\n",
    "    'поймали',\n",
    "    'большую',\n",
    "    'рыбу',\n",
    "]\n",
    "\n",
    "# Создать стеммер\n",
    "snowball = SnowballStemmer('russian')\n",
    "\n",
    "# Применить стеммер\n",
    "[snowball.stem(word) for word in tokenized_words_rus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделение основ слов сводит слово к его основе путем выявления и отсечения окончаний и аффиксов (например, герундийных форм в англ. языке), сохраняя при этом корневой смысл слова. Например, слова $tradition$  и $traditional$ имеют $tradit$ в качестве основы, указывая на то, что хотя это и разные слова, они представляют одно и тоже обще понятийное пространство. Используя текстовые данные можно преобразовать слова во что-то немее читаемое, но близкое к базовому значению, и следовательно более подходящее для сопоставления наблюдений. Класс NLTK `PorterStemmer` реализует широко используемый алгоритм выделения основ Портера, который отсекаетили заменяет общеупотребительные суффиксы для получения основы слова.\n",
    "##### Дополнительные материалы\n",
    "* [Алгоритм выделения основ слов (стеммер) Портера](https://tartarus.org/martin/PorterStemmer/)\n",
    "* [Алгоритм выделения основ слов русского языка (стеммер) Snowball](http://snowball.tartarus.org/algorithms/russian/stemmer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Лемматизация слов\n",
    "##### Задача\n",
    "Даны лексимизированные слова, требуется собрать их в синонимические ряды\n",
    "##### Решение\n",
    "Используем класс NLTK `WordNetLemmatizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'go', 'go', 'be', 'be', 'be', 'be', 'be']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Создать лексемы слов\n",
    "tokenized_words = [\n",
    "    'go',\n",
    "    'gone',\n",
    "    'went',\n",
    "    'am',\n",
    "    'be',\n",
    "    'is',\n",
    "    'are',\n",
    "    'were'\n",
    "]\n",
    "\n",
    "# Создать леммтизатор\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Применить лемматизатор\n",
    "[lemmatizer.lemmatize(word, pos='v') for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично выделению основы слова, лемматизация также собирает разные флективные формы слова в группу, чтобы они могли анализироваться как одинаковые.\n",
    "\n",
    "В отличие от выделения основы слова, данная процедура более запутанная и требует некоторых дополнительных знаний, нпаример, правильная метка части речи связанная с каждым словом, которое подлежит лемматизации. Результат лематизации называется леммой и в сущности является словом в буквальном смысле. Простой подход с отсечением суффиксов для лемматизации работать не будет, потому что, например, некоторые формы неправильного глагола в английском языке имеют совершенно другую морфологию, чем их лемма. Например, $go, goes, going, went$ и все они должны бысть сопоставлены глаголу $go$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Разметка на части речи\n",
    "##### Задача\n",
    "Даны текстовые данные, и требуется пометить каждое слово или сомвол своей частью речи\n",
    "##### Решение\n",
    "Используем предварительно натренированный разметчик частей речи библиотеки NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Создать текст\n",
    "text_data = 'Chris loved outdoor running'\n",
    "\n",
    "# Использовать предварительно натренированный разметчик частей речи\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# Показать части речи\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получен список кортежей со словом и тегом части речи. NLTK использует метки частей речи текстового корпуса Penn Treebank. Примедем несколько примеров этих меток:\n",
    "\n",
    "Метка | Часть речи\n",
    "----- | -----------\n",
    "NNP   | Имя собственное, единственное число\n",
    "NN    | Существительное, единственное число или неисчисляемое\n",
    "RB    | Наречие\n",
    "VBD   | Глагол, прошедшее время\n",
    "VBG   | Глагол, герундий или причастие настоящего времени\n",
    "JJ    | Прилагательное\n",
    "PRP   | Личное местоимение\n",
    "\n",
    "После того, как текст был помечен, метки можно использовать для поиска определенных частей речи. Например, вот все существительные:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Отфильтровать слова\n",
    "[word for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более реалистичной является ситуация, когда есть данные, где каждое наблюение содержит твит, и необходимо преобраховать эти предложения в признаки отдельных частей речи. Напрмер, признак $1$, если присутствует собственное существительное, и $0$ в противном случае:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Создать текст\n",
    "tweets = [\n",
    "    'I am eating a burrito for breakfast',\n",
    "    'Political science is an amzing field',\n",
    "    'San Francisco is a awesome city',\n",
    "]\n",
    "\n",
    "# Создать список\n",
    "tagged_tweets = []\n",
    "\n",
    "# Пометить каждое слово и каждый твит\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "    \n",
    "# Применить кодирование с одним активным состоянием, чтобы конвертировать метки в признаки\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применив атрибут `classes_` можно увидеть, что каждый признак является меткой части речи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Показать имена признаков\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если исследуемый текст написан на английском языке и не относится к специализированной теме, например, к медицине, то самым простым решением являтся использование предварительно натренированного разметчика на части речи NLTK. Однако, если функция `pos_tag` не очень точна, то NLTK также дает возможность натренировать собственный разметчик. Основным недостатком тренировки разметчика является то, что нам нужен большой текстовый корпус, где метка каждого слова известна. Создание такого помеченного корпуса, очевидно, является трудоемкой задачей, и вероятно будет последним средством, к которому стоит обращаться.\n",
    "\n",
    "С учетом вышесказанного, натренируем разметчик на стандартном корпусе американского английского языка университета Брауна (Brown Corpus). Используем $n$-граммный разметчик с откатом, где $n$ — это количество предыдущий слов, которые необходимо учитывать во время предсказания метки части речи. Сначала принимается во внимание два предыдущих слова, используя триграммный разметчик `TrigramTagger`; если два слова отсутствуют, то принимается во внимание метка предыдущего слова, используя биграммный разметчик `BigramTagger`. Наконец, если и это не удается, используется униграммный разметчик `UnigramTagger`. Для того, чтобы проверить точность разметчика, необходимо разбить текстовые данные на две части. На одной будет происходить тернировка разметчика, на другой – тестирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "# Получить текст из стандартного текстового корпуса, разбитого на предложения\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "# Разбить на 4000 предложений для тренировки и 4000 – для тестирования\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "\n",
    "# Создать разметчик с откатом\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "\n",
    "# Показать точность\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Дополнительные материалы\n",
    "* [Алфавитный список частей речи текстового корпуса Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "* [Стандартный корпус современного американского варианта английского языка университета Brown](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
    "* [Определение частей речи слов в русском языке](https://habr.com/ru/post/125988/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Кодирование текста в качестве мешка слов\n",
    "##### Задача\n",
    "Даны таекстовые данные, требуется создать набор признаков, указывающих на количество вхождений определенного слова в текст\n",
    "##### Решение\n",
    "Используем класс `CountVectorizer` библиотеки Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загурзить библиотеки\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Создать текст\n",
    "text_data = np.array([\n",
    "    'Бразилия — моя любовь! Бразилия!',\n",
    "    'Швеция – лучше',\n",
    "    'Россия бьет обоих!',\n",
    "])\n",
    "\n",
    "# Создать матрицу признаков\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# Показать матрицу признаков\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот результат является разряженным массивом, который необходим, когда объем текста большой. В данном игрушечном случае для просмотра матрицы частности слов по каждому наблюдения можно применить метод `toarray()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для просмотра слов, связанного с каждым признаком, можно применить метод `get_features_names()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['бразилия', 'бьет', 'лучше', 'любовь', 'моя', 'обоих', 'россия', 'швеция']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Показать имена признаков\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот результат может сбить с толку, поэтому для ясности покажем эту матрицу как таблицу, где имена столбцов будут именами признаков, а каждая строка – наблюдением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>бразилия</th>\n",
       "      <th>бьет</th>\n",
       "      <th>лучше</th>\n",
       "      <th>любовь</th>\n",
       "      <th>моя</th>\n",
       "      <th>обоих</th>\n",
       "      <th>россия</th>\n",
       "      <th>швеция</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   бразилия  бьет  лучше  любовь  моя  обоих  россия  швеция\n",
       "0         2     0      0       1    1      0       0       0\n",
       "1         0     0      1       0    0      0       0       1\n",
       "2         0     1      0       0    0      1       1       0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=bag_of_words.toarray(), columns=count.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Олним из наиболее распространенных методов преобразования текста в признаки является использование модели *мешка слов*. Эти модели выводят признак для каждого уникального слова в текстовых данных, при этом каждый признак содержит Количество вхождений в наблюдениях. Поскольку модель мешка слов создает признаки для каждого уникального слова, на практике результирующая матрица может содержать тысячи признаков. В таких случаях необходимо уменьшать объем данных.\n",
    "\n",
    "Для этого можно воспользоваться общей особенностью мешка слов. Большинство слов, скорее всего, не встреаются в большинстве наблюдений, и поэтому матрицы признаков на основе мешка слов будут в качестве значений содержать нули. Такие матрицы называются *разряженными*. Вместо того, чтобы хранить все значения, можно хранить только ненулевые значения, исходя из того, что все остальные равны $0$. При наличии крупных матриц признаков – такой подход может сэкономить оперативнуб память. Одной из особенностей векторизатора частотностей `CountVectorizer` является то, что результатом по умточанию будет разряженная матрица.\n",
    "\n",
    "Класс `CountVectorizer` сопровождается рядом полезных параметров, которые упрощают создание матриц признака на основе мешка слов.\n",
    "* Во-первых, хотя каждый признак является словом, это не обязательно. Вместо этого можно установить, чтобы каждый признак был комбинацией двух слов (2-граммы) или даже трех слов (3-граммы). Парметр, `ngram_range=` устанавливает минимальный и максимальный размеры $n$-грамм. Например, параметр `ngram_range=(2,3)` вернет все 2-граммы и 3-граммы.\n",
    "* Во-вторых, можно легко удалить слово с низкой информацией, используя стоп-слова в параметре `stop_words=`, либо с помощью встроенного, либо с помощью собственного псика стоп-слов.\n",
    "* В-третьих, можно ограничить слова или фразы, которые необходимо рассмотреть, заданные списком слов, используя параметр `vacabulary=`. Например, можно создать матрицу признаков на сонове мешка слов только для вхождений названий стран:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать матрицу признаков с аргументами\n",
    "count_2gram = CountVectorizer(ngram_range=(1, 2),\n",
    "                             stop_words='english',  # или использовать собственный список\n",
    "                             vocabulary=['бразилия'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "\n",
    "# Взглянуть на матрицу признаков\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'бразилия': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на 1-граммы и 2-граммы\n",
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Дополнительные материалы\n",
    "* [\"N-граммы\"](https://ru.wikipedia.org/wiki/N-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0)\n",
    "* [\"Мешок слов встречается со стаканом попкорна\"](https://www.kaggle.com/c/word2vec-nlp-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Взвешивание важности слов\n",
    "##### Задача\n",
    "Требуется мешок слов, но сословами, взвешенными по их важности для наблюдения\n",
    "##### Решение\n",
    "Для сравнения частоты слов в документе с частотой слов в других документах используется статистическая мера словарной частоты (tf-idf).\n",
    "\n",
    "Используем класс `TfidVectorizer` библиотеки Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Создать матрицу признаков на основе меры tf-idf\n",
    "tfdif = TfidfVectorizer()\n",
    "feature_matrix = tfdif.fit_transform(text_data)\n",
    "\n",
    "# Показать матрицу признаков на основе меры tf-idf\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в предыдущем примере получется разряженная матрица, однако если требуется взглянуть на рещультат, как на плотную матрицу, можно использовать метод `toarray()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81649658, 0.        , 0.        , 0.40824829, 0.40824829,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.70710678, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.70710678],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.57735027, 0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Показать плотную матрицу признаков на основе tf-idf\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Атрибут `vocabulary_` показывает слово каждого признака:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'бразилия': 0,\n",
       " 'моя': 4,\n",
       " 'любовь': 3,\n",
       " 'швеция': 7,\n",
       " 'лучше': 2,\n",
       " 'россия': 6,\n",
       " 'бьет': 1,\n",
       " 'обоих': 5}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdif.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем чаще слово поялятся в документе, тем более вероятно, что оно важно для этого документа. Например, если слово *\\\"экономика\\\"* встречается часто, то это свидетельствет о том, что это документ может касаться экономики. Данная мера частотной встречаемости слова в документе назывется *словарной частостой (tf)*.\n",
    "\n",
    "Напротив, если слово появляется во многих документах, оно скорее всего , менее важно для любого отдельного документа. Напрмер, если в каждом документе встречается слово *\\\"после\\\"*, то это слово, скорее всего, не представляет важность. Такая мера частотной встречаемости слова во всех документах называется *документной частотой (df)*. \n",
    "\n",
    "Объединив эти два статистических показателя, можно определить оценку каждому слову, тем самым показывая, насколько важно слово в документе. В частности, необходимо умножить $tf$ на обратную частоту документа $idf$:\n",
    "$$tf-idf(t, d) = tf(t, d) \\cdot idf(t, d)$$\n",
    "где $t$ – слово, $d$ – документ.\n",
    "\n",
    "Существует ряд отличий в том, как рассчитываются $tf$ и $idf$. В библиотеке Scikit-learn $tf$ – это просто количество раз, когда слово появляется в документе, а $idf$ – рассчитывается следующим образом:\n",
    "$$idf(t) = log \\frac{1 + n_d}{1 + df(d, t} + 1$$\n",
    "где $n_d$ – это количество документов, $df(d, t)$ –документная частота слова $t$ (т.е. количество документов, в которых появляется это слово).\n",
    "\n",
    "По умолчанию Scikit-learn затем нормализует векторы $tf-idf4$, используя евклидову норму $L^2$, чем выше результирующее значение, тем важнее слово для документа.\n",
    "\n",
    "##### Дополнительные материалы\n",
    "* [Документация библиотеки Scikit-learn: вщвешивание на основе весогового коэффициента tf-idf](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
